\pagebreak

\section{Proposed OHGesture Model}
\label{sec:ohgesture}

\begin{figure}[h]
	\centering
		\includegraphics[width=\linewidth]{AllStage}
	\caption{Overview of the OHGesture model}
	\label{fig:TrainingAndSampling}
\end{figure}

The proposed model \textbf{OHGesture} (\textbf{O}pen \textbf{H}uman \textbf{Gesture} Generation) in this thesis is based on the \textbf{DiffuseStyleGesture} model \cite{yang2023diffusestylegesture}, which applies the Diffusion model \cite{ho2020denoising} with conditional guidance \cite{ho2022classifier} (Classifier-Free Diffusion Guidance) to control features during the denoising process.

The similarities and differences of applying the diffusion model to the gesture generation task compared to image generation are as follows:

\vspace{10pt}

\textbf{Similarities}
\begin{itemize}
	\item Uses the Diffusion model (\autoref{sec:summary_diffusion}) on gesture data $\bx^{1:M \times D}$, with $M$ temporal frames and $D=1141$ representing motion coordinates per frame (analogous to image width and height).
	\item Uses conditional Diffusion (\autoref{subsec:DiffusionCondition}) with $\bx_0$ objective (\autoref{subsec:X0Objective}).
	\item In stages \textit{4. Feature Encoding} and \textit{6. Feature Decoding} in \autoref{fig:CommonStage}, the model uses a latent vector of dimension $256$.
\end{itemize}

\textbf{Differences}
\begin{itemize}
	\item Conditional gesture generation:
	\begin{itemize}
		\item Emotional condition: $c = \big[ \mathbf{s}, \mathbf{e}, \mathbf{a}, \mathbf{v} \big]$ and $c_{\varnothing} = \big[ \varnothing, \varnothing, \mathbf{a}, \mathbf{v}\big]$.
		\item Emotion state interpolation between $\mathbf{e}_1, \mathbf{e}_2$ using: $c = \big[ \mathbf{s}, \mathbf{e}_1, \mathbf{a}, \mathbf{v} \big]$ and $c_{\varnothing} = \big[ \mathbf{s}, \mathbf{e}_2, \mathbf{a}, \mathbf{v} \big]$.
	\end{itemize}
	\item In stage \textit{5. Feature Fusion} \autoref{fig:CommonStage}, the model uses Self-Attention: learning the relationship between emotions, seed gestures, and each frame (similar to DALL-E 2's text-image alignment).
	\item In stage \textit{5. Feature Fusion} \autoref{fig:CommonStage}, the model concatenates speech and text (analogous to ControlNet's pixel-wise condition).
\end{itemize}

Here, $\bx_0$ is a sequence of $M$ gesture frames $\mathbf{x} \in \mathbb{R}^{1:M \times D}$ ($D = 1141$), with condition $c = [\mathbf{s}, \mathbf{e}, \mathbf{a}, \mathbf{v}]$ including seed gesture $\mathbf{s}$, emotion $\mathbf{e}$, speech $\mathbf{a}$ corresponding to the gesture, and text $\mathbf{v}$.

The model's objective is to learn parameters $\theta$ of the generative function $G_{\theta}$ with inputs being the noisy gesture matrix $\bx_t \in \mathbb{R}^{1:M \times D}$, timestep $t$, and condition $c$. An overview of the proposed \textbf{OHGesture} model is illustrated in \autoref{fig:TrainingAndSampling}. As with standard diffusion models, it includes two processes: the diffusion process $q$ and the denoising process $p_{\theta}$ with weights $\theta$. The \textit{1. Preprocessing} stage will be presented in \autoref{sec:Preprocessing}.


\subsection{Feature Processing Stage}

In the \textit{Stage 2: Feature Processing} step (\autoref{fig:CommonStage}), the goal is to convert raw input data into matrices or vectors suitable for input into the model.

\begin{itemize}
    \item \textbf{Text} $\mathbf{v} \in \mathbb{R}^{1:M \times 300}$:  
    As discussed in \autoref{subsec:TypicalMethod}, methods such as \textit{MDM} \cite{tevet2022human} and \textit{DiffuseStyleGesture+} \cite{yang2022DiffuseStyleGestureplus} use gesture-descriptive prompts, similar to those used in Midjourney, as input for the model. However, such prompts are mainly used to cluster gestures rather than align them semantically. 

    In contrast, this thesis treats text as a semantic feature that aligns specific segments of text with corresponding gesture segments for digital human generation. Therefore, the contribution in this stage is to use speech data that has already been preprocessed (see \autoref{sec:Preprocessing}) to obtain transcribed text. The FastText model \cite{bojanowski2017enriching} is used to embed this text into vectors, which are then temporally aligned with the number of gesture frames, resulting in a text matrix $\mathbf{v} \in \mathbb{R}^{1:M \times 300}$. For segments without text, zero vectors are used; for segments with vocabulary, the corresponding embedding is assigned for each gesture frame.

    \item \textbf{Speech} $\mathbf{a} \in \mathbb{R}^{1:M \times 1024}$:  
    All speech data in `.wav` format is downsampled to $16~\mathrm{kHz}$. The speech corresponding to the gesture segment (4 seconds) is extracted as a waveform vector $\mathbf{a} \in \mathbb{R}^{64000}$. Following DiffuseStyleGesture, this thesis uses the pre-trained WavLM Large model \cite{Chen_2022} to embed the raw waveform into a high-dimensional vector representing acoustic features. Linear interpolation is then applied to temporally align the latent features from WavLM at $20~\text{fps}$, resulting in the speech matrix $\mathbf{a} \in \mathbb{R}^{1:M \times 1024}$.

    \item \textbf{Emotion} $\mathbf{e} \in \mathbb{R}^{6}$:  
    Emotion is represented by one of six classes: $\texttt{Happy}$, $\texttt{Sad}$, $\texttt{Neutral}$, $\texttt{Old}$, $\texttt{Relaxed}$, and $\texttt{Angry}$.  
    Each label is encoded using one-hot encoding to form the vector $\mathbf{e} \in \mathbb{R}^{6}$.

    \item \textbf{Seed Gesture} $\mathbf{s} \in \mathbb{R}^{1:N \times D}$:  
    This is the initial gesture sequence composed of $N=8$ frames, with each frame containing joint data for 75 joints. These are processed according to the formula in \autoref{eq:gesturevector} to yield a $D=1141$-dimensional vector.

    \item \textbf{Ground Truth Gesture} $\mathbf{x}_{0} \in \mathbb{R}^{1:M \times D}$:  
    This is the ground-truth gesture sequence of $M = 80$ frames (corresponding to 4 seconds at $20~\text{fps}$), which is preprocessed to form the matrix $\mathbf{x}_{0} \in \mathbb{R}^{1:M \times D}$.
\end{itemize}

\subsection{Feature Extraction Stage}
\label{subsec:feature_extraction}

In the \textit{Stage 3: Feature Extraction} step (\autoref{fig:CommonStage}), the goal is to convert the input matrices into latent vectors that capture the semantic content of each modality. This is done by passing the feature data through linear transformation layers or Multilayer Perceptrons (MLPs).


\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{OHGesture}
	\caption{The Model Architecture in OHGesture}
	\label{fig:OHGesture}
\end{figure}
\vspace{-10pt}

\begin{itemize}
    \item \textbf{Timestep $\mathbf{T} \in \mathbb{R}^{256}$}: The timestep at each process step $t \in [0, T]$ enables the model to generalize the denoising process across different time steps. The objective is for the model to learn how the values should change with respect to $t$ in order to accurately predict $\bx_0$. The timestep $t$ is initialized using sinusoidal positional encoding: $\text{PE}(t) = \left[ \sin{\left(\frac{t}{10000^{2i / d}}\right)}, \cos{\left(\frac{t}{10000^{2i / d}}\right)} \right]$, and then passed through a Multilayer Perceptron (MLP) to obtain the vector $\mathbf{T} \in \mathbb{R}^{256}$.

    \item \textbf{Speech $\mathbf{A} \in \mathbb{R}^{1:M \times 64}$}: The speech feature matrix $\mathbf{a} \in \mathbb{R}^{1:M \times 1024}$ is passed through a linear layer to reduce its dimensionality to a 64-dimensional feature vector, resulting in the matrix $\mathbf{A} \in \mathbb{R}^{1:M \times 64}$.

    \item \textbf{Text $\mathbf{V} \in \mathbb{R}^{1:M \times 64}$}: After preprocessing described in \autoref{sec:Preprocessing}, the text is aligned to match the number of frames $M$, resulting in matrix $\mathbf{v} \in \mathbb{R}^{1:M \times 300}$. It is then passed through a linear transformation to reduce feature dimensionality, yielding the final matrix $\mathbf{V} \in \mathbb{R}^{1:M \times 64}$, aligned with the speech matrix.

    \item \textbf{Seed Gesture $\mathbf{S} \in \mathbb{R}^{192}$}: The seed gesture $\mathbf{s} \in \mathbb{R}^{1:N \times D}$ is processed through a linear transformation layer to obtain the vector $\mathbf{S} \in \mathbb{R}^{192}$. During training, $\mathbf{S}$ is also passed through a random masking layer to randomly mask segments of gesture frames in $N$ frames. This allows the model to learn how missing frames impact the final predicted gestures.

    \item \textbf{Emotion $\mathbf{E} \in \mathbb{R}^{64}$}: The emotion vector $\mathbf{e} \in \mathbb{R}^{6}$ is passed through a linear transformation layer to produce the feature vector $\mathbf{E} \in \mathbb{R}^{64}$. This is designed to be concatenated with the seed gesture vector $\mathbf{S} \in \mathbb{R}^{192}$ to form a 256-dimensional latent vector.

    \item \textbf{Noisy Gesture $\mathbf{x}_{T} \in \mathbb{R}^{1:M \times D}$}: During training, $\mathbf{x}_t$ is the noisy gesture that shares the same dimensionality as the original gesture $\mathbf{x}_0$ and is sampled from a standard normal distribution $\mathcal{N}(0, \mathbf{I})$. The initial noisy gesture $\mathbf{x}_T$ is drawn from a Gaussian distribution, and the subsequent $\mathbf{x}_t, \, t < T$ are produced through iterative noising steps, as illustrated in \autoref{fig:TrainingAndSampling}.
%   Afterward, the dimensionality is adjusted to 256 using a linear transformation layer.
\end{itemize}


\subsection{Feature Encoding Stage}

In the \textit{Stage 4: Feature Encoding} step (\autoref{fig:CommonStage}), the objective is to reduce the dimensionality of the input data to a lower latent space, in order to alleviate computational overhead and avoid the explosion of processing complexity.

The primary data used in the diffusion process is the gesture sequence $\bx_t \in \mathbb{R}^{1:M \times D}$. As illustrated in \autoref{fig:OHGesture}, the gesture sequence with size $M \times D$ is passed through a linear transformation layer $\operatorname{Linear}_{\theta}$ to produce the matrix $\mathbf{X} \in \mathbb{R}^{1:M \times 256}$. This dimensionality reduction of $\bx$ is performed prior to passing the gesture sequence through the Cross-Local Attention and Transformer Encoder layers, in order to compute correlations across multiple modalities.

\subsection{Feature Fusion Stage}

In the \textit{Stage 5: Feature Fusion} step (\autoref{fig:CommonStage}), the goal is to compute inter-feature correlations using concatenation, addition, or attention-based mechanisms.

First, the seed gesture vector $\mathbf{S} \in \mathbb{R}^{192}$ and the emotion vector $\mathbf{E} \in \mathbb{R}^{64}$ are concatenated to form a single vector of size $256$, since $256$ is the chosen hidden dimensionality for computing feature correlations. This vector is then added to the timestep vector $\mathbf{T}$ to form the final vector $\mathbf{z}_{tk} \in \mathbb{R}^{256}$.


\begin{equation}
	\label{eq:ConditionConcat}
	\mathbf{z}_{tk} = \operatorname{concat }(\mathbf{E}\ || \  \mathbf{S}) + \mathbf{T}
\end{equation}

The feature fusion process of $\mathbf{z}_{tk}$ is illustrated in \autoref{fig:FeatureFusion}.

\subsubsection{Frame-wise Feature Integration}

Next, $\mathbf{z}_{tk} \xrightarrow{\operatorname{replicate}} \mathbf{Z}$ is replicated $M$ times to match the dimensionality of $M$ frames, resulting in the matrix $\mathbf{Z} \in \mathbb{R}^{1:M \times 256}$, as shown in \autoref{fig:OHGesture}.

The speech feature matrix $\mathbf{A}$ and the text feature matrix $\mathbf{V}$ are then added together to produce a matrix that combines both modalities. This combined matrix is then concatenated with the gesture feature matrix $\mathbf{X}$. Finally, the result is concatenated with matrix $\mathbf{Z}$ to obtain the full feature matrix $\mathbf{M}$.


\begin{equation}
	\label{eq:FrameConcat}
	\mathbf{M} = \operatorname{concat}( \mathbf{Z}\  || \   \operatorname{concat}(\mathbf{X}\ || \  (\mathbf{V} + \mathbf{A}) ) )
\end{equation}

The matrix $\mathbf{M} \in \mathbb{R}^{1:M \times P}$, as shown in \autoref{eq:FrameConcat}, represents the frame-wise feature matrix from frame $1$ to frame $M$, where each frame has dimensionality $P$, which is the sum of all concatenated feature vectors. Given $\mathbf{X} \in \mathbb{R}^{1:M \times 256}$, $\mathbf{Z} \in \mathbb{R}^{1:M \times 256}$, and $\mathbf{A}, \mathbf{V} \in \mathbb{R}^{1:M \times 64}$, the resulting dimensionality is $P = 256 + 256 + 64$.

Subsequently, the matrix $\mathbf{m}$ is passed through a linear transformation to reduce its dimensionality from $P$ to $256$, resulting in the matrix $\mathbf{m}_{t} \in \mathbb{R}^{1:M \times 256}$.

\begin{equation}
	\label{eq:FeatureDimensionReducion}
	\mathbf{m}_{t} = \operatorname{Linear}_{\theta}( \mathbf{M} )
\end{equation}

\subsubsection{Attention Mechanism in the Feature Integration Process}

In the proposed model, the attention mechanism \cite{vaswani2017attention} is employed to integrate features. The objective of applying attention is to capture the correlation between individual frames in the sequence. The attention mechanism is utilized in both the Cross-Local Attention and the Self-Attention layers within the Transformer Encoder.

The attention mechanism is formulated as follows:

\begin{equation} \label{eq:attention}
	\operatorname{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}, \mathbf{Mask})=\operatorname{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^{T}+\mathbf{Mask}}{\sqrt{C}}\right) \mathbf{V}
\end{equation}

In the Attention formula above, $\mathbf{Q}$ (Query), $\mathbf{K}$ (Key), and $\mathbf{V}$ (Value) are matrices obtained by passing the input through linear transformation matrices: $\mathbf{Q} = {X} \mathbf{W}_Q$, $\mathbf{K} = {X} \mathbf{W}_K$, $\mathbf{V} = {X} \mathbf{W}_V$. The input $X$ is a matrix representing a sequence of $M$ frames, where each frame is a concatenated vector composed of various feature vectors, including seed gestures, text, speech, emotion, and the gesture $\bx_t$ that the thesis aims to denoise. The term $\sqrt{C}$ is a normalization \textbf{c}onstant that accounts for the dimensionality of the matrices. 

The Local-Cross Attention mechanism is controlled to focus only on local motion features of gestures and features in neighboring frames.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{CrossLocalAttention}
	\caption{Attention Mechanism in Transformer Encoder and Cross-Local Attention}
	\label{fig:CrossLocalAttention}
\end{figure}

The Attention mechanism functions like a dictionary, where the final retrieved information is the matrix $\mathbf{V}$ (value), while $\mathbf{Q}$ (query) represents the keyword being searched for, and $\mathbf{K}$ (key) is the set of keywords in the lookup dictionary. The Attention process computes the similarity between \( \mathbf{Q} \) and \( \mathbf{K} \) to determine the weights for the values in \( \mathbf{V} \).

The final result is a weighted combination of the values in \( \mathbf{V} \), where the values corresponding to keys most similar to the query receive higher weights. $\mathbf{M}$ is the mask used to perform local attention. The Cross-Local Attention mechanism is illustrated on the right in \autoref{fig:CrossLocalAttention}, while the Transformer Encoder layer uses Self-Attention shown on the left.

\subsubsection{Combining Local Features with Cross-Local Attention}

%The matrix $\mathbf{m}_{t} \in \mathbb{R}^{1:M \times 256}$ is then passed through the Cross-Local Attention layer to compute the correlation between local features.

The matrix $\mathbf{m}_{t} \in \mathbb{R}^{1:M \times 256}$ is then passed through the Cross-Local Attention layer to compute the correlations between local features.
% resulting in the matrix $\mathbf{h}_t \in \mathbb{R}^{1:M \times D}$ :


\begin{equation}
	\mathbf{h}_{t}  = \operatorname{Linear}_{\theta}  ( \operatorname{Cross-Local\ Attention}( \mathbf{m}_{t}) )
	\label{eq:CrossLocalAttention}
\end{equation}


% (\operatorname{Query}) (\operatorname{Key}) (\operatorname{Value})
Cross-local Attention is performed with $\mathbf{Q} = \mathbf{K} = \mathbf{V} = \mathbf{m}_{t}$.
Following the idea of the Routing Transformer method \cite{roy2021efficient}, Cross-local Attention highlights the importance of constructing intermediate feature-vector representations before passing them through the Transformer Encoder layer, as shown in \autoref{fig:OHGesture}.
The feature vectors are augmented with a relative position-encoding vector \textbf{RPE} (\textbf{R}elative \textbf{P}osition \textbf{E}ncoding) to preserve temporal ordering before entering the Cross-Local Attention layer.

After Cross-Local Attention, the model is forwarded through a linear layer, as in \autoref{eq:CrossLocalAttention}, to align with the $M$ frames and obtain the matrix $\mathbf{h}_t \in \mathbb{R}^{1:M \times D}$.

\subsubsection{Global Feature Fusion with the Transformer Encoder}

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.42\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{FeatureConcatenate}
		\caption{Feature concatenation for $\mathbf{z}_{tk}$}
		\label{fig:FeatureFusion}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.55\textwidth}
		\centering
		\includegraphics[width=0.85\textwidth]{FeatureFusion}
		\caption{Frame-wise feature-fusion process}
		\label{fig:ZToken}
	\end{subfigure}
\end{figure}

Following MDM \cite{tevet2022human}, the vector $\mathbf{z}_{tk}$ is the first token that encodes information for the entire frame sequence, analogous to the $\texttt{CLS}$ token in BERT \cite{devlin2019bertpretrainingdeepbidirectional}, which summarizes an entire text segment.
Here, the thesis uses $\mathbf{z}_{tk}$, with $\mathbf{z}_{tk} \in \mathbb{R}^{256}$ (\autoref{eq:ConditionConcat}), as the first token representing global features for the whole sequence of $M$ frames.

\begin{equation}
	\mathbf{X}_{0} = \operatorname{Transformer\ Encoder}\bigl(\operatorname{concat}(\mathbf{z}_{tk} \;\|\; \mathbf{h}^{1:M}_{t})\bigr)
	\label{eq:TransformerEncoder}
\end{equation}

The vectors $\mathbf{h}_t$ represent the sequence of $M$ frames. Similar to Reformer \cite{kitaev2020reformer}, before entering the Self-Attention layer of the Transformer Encoder, the model employs Relative Position Encoding (RPE) instead of absolute position encoding, improving efficiency on long sequences.
Within the Transformer Encoder layer \cite{vaswani2017attention}, relationships among the data sequences are computed.
The Transformer Encoder applies the same self-attention mechanism as in \autoref{eq:attention} but without the $\mathbf{Mask}$, enabling correlations across the entire sequence to be captured.

\subsection{Feature Decoding Stage}

In stage \textit{6. Feature Decoding} (\autoref{fig:CommonStage}), once feature correlations are computed, the goal is to upsample the data back to its original dimensionality.

As illustrated in \autoref{fig:OHGesture}, the latent matrix $\mathbf{X}_{0}$, after passing through the Transformer Encoder to capture correlations among heterogeneous data types, is fed into a linear projection layer
$\hat{\mathbf{x}}_{0} = \operatorname{Linear}_{\theta}(\mathbf{X}_{0})$
to restore the latent matrix to its original size, yielding $\hat{\mathbf{x}}_{0} \in \mathbb{R}^{1:M \times D}$.

The final rendering step is presented in \autoref{sec:Render}.

\subsection{Emotion Control in Gesture Generation}

The preceding steps enable the model to learn gesture generation. To incorporate emotions across different contexts, each emotion is parameterized and varied so that the predictions faithfully express the designated affect.

Analogous to conditional denoising models \cite{ho2022classifier, tevet2022human}, the thesis uses the condition vector
$c = [\mathbf{s}, \mathbf{e}, \mathbf{a}, \mathbf{v}]$,  
where $\mathbf{s}$ is the seed gesture, $\mathbf{e}$ the emotion, $\mathbf{a}$ the associated speech, and $\mathbf{v}$ the text.
The conditional diffusion model injects $c$ at every timestep $t$ in the denoising network $\text{G}_\theta(\bx_{t}, t, c)$, with
$c_{\varnothing} = [\varnothing, \varnothing, \mathbf{a}, \mathbf{v}]$ (unconditional)
and $c = [\mathbf{s}, \mathbf{e}, \mathbf{a}, \mathbf{v}]$ (conditional).
A random mask applied to the seed-gesture and emotion vectors conveniently switches labels, allowing optimization under diverse conditions.

\begin{equation} \label{eq:denoise}
\hat{\bx}_{0\,c,c_{\varnothing},\gamma}
    = \gamma\, G(\bx_{t}, t, c) + (1-\gamma)\, G(\bx_{t}, t, c_{\varnothing})
\end{equation}

Classifier-free guidance \cite{ho2022classifier} further enables interpolation between two emotions
$\mathbf{e}_1$ and $\mathbf{e}_2$ by setting
$c = [\mathbf{s}, \mathbf{e}_{1}, \mathbf{a}, \mathbf{v}]$ and
$c_{\varnothing} = [\mathbf{s}, \mathbf{e}_{2}, \mathbf{a}, \mathbf{v}]$:
\[
\hat{x}_{0\,\gamma, c_{1}, c_{2}}
    = \gamma\, G(x_{t}, t, c_{1}) + (1-\gamma)\, G(x_{t}, t, c_{2}).
\]

\subsection{Training Procedure}

\begin{algorithm}[h]
	\caption{Training in OHGesture}
	\label{alg:trainingohgesture}
	\setlength{\baselineskip}{10pt}
	\begin{enumerate}
		\item Pre-compute $\gamma$, $\sqrt{\alpha_t}$, $\sqrt{1-\alpha_t}$, $\sqrt{\bar{\alpha}_t}$, and random noise $\boldsymbol{\epsilon}_t$ for each timestep $t: 1 \rightarrow T$. Define the noise schedule $\{\alpha_t \in (0,1)\}_{t=1}^T$.
		\item Sample the initial label $\mathbf{x}_0$ from the normalized data distribution.
		\item Randomly generate Bernoulli masks
		      $c_{1} = [ \mathbf{s}, \mathbf{e_1}, \mathbf{a}, \mathbf{v} ]$,
		      $c_{2} = [ \mathbf{s}, \mathbf{e_2}, \mathbf{a}, \mathbf{v} ]$, or
		      $c_{2} = [ \varnothing, \varnothing, \mathbf{a}, \mathbf{v} ]$.
		\item Add noise to obtain the noisy gesture $\mathbf{x}_t$:
		      \[
		      \mathbf{x}_t = \sqrt{\bar{\alpha}_t}\,\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\,\boldsymbol{\epsilon}_t.
		      \]
		\item Sample $t$ \textbf{uniformly} from $[1, T]$.
		\item Given $\mathbf{x}_t$, $t$, and masks $c_1$, $c_2$, predict the gesture sequence:
		      \[
		      \hat{\mathbf{x}}_{0\,\gamma,c_{1},c_{2}}
		          = \gamma\, G_{\theta}(\mathbf{x}_{t}, t, c_{1})
		          + (1-\gamma)\, G_{\theta}(\mathbf{x}_{t}, t, c_{2}).
		      \]
		\item Compute the loss and gradient to update $\theta$:
		      \[
		      \mathcal{L}^t
		          = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}_t}
		            \bigl[\operatorname{HuberLoss}(\mathbf{x}_0, \hat{\mathbf{x}}_0)\bigr].
		      \]
		\item Repeat from step 6 until convergence, obtaining the optimal parameters $\theta'$.
	\end{enumerate}
\end{algorithm}

\autoref{alg:trainingohgesture} trains the OHGesture model by first computing the required values and hyper-parameters—$\gamma$, $\sqrt{\alpha_t}$, $\sqrt{1-\alpha_t}$, $\sqrt{\bar{\alpha}_t}$, and $\boldsymbol{\epsilon}_t$—for every timestep $t$ (1 … $T$).  
The initial label $\mathbf{x}_0$, representing the ground-truth gesture, is drawn from the normalized data distribution.  
Random Bernoulli masks $c_1$ and $c_2$ emulate different conditions (gesture, emotion, speech, or text), with one mask possibly lacking emotion information.  
Noise is then added to create the noisy gesture $\mathbf{x}_t$.  
A timestep $t$ is sampled uniformly, and $\mathbf{x}_t$ with the masks is fed into the model to predict the original gesture sequence as a weighted combination of conditional outputs.  
The Huber loss between ground-truth and prediction is used to update $\theta$.  
This cycle repeats until the model converges, yielding the optimal parameters $\theta'$.

\subsection{Sampling Process}

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{OnlineAndOffline}
	\caption{Offline (Training) and Online (Inference) Phases}
	\label{fig:OnlineAndOffline}
\end{figure}

To generate gestures of arbitrary length, the original sequence is segmented into clips of length $M$.
During training, the seed gesture can be chosen by randomly selecting a gesture from the dataset or by averaging the clipped segments—here, the mean rotation angles are used.  
Generated frames are processed sequentially, with the last $N=8$ frames taken as the seed for the next iteration.  
For each clip, the gesture $\bx_{t}$ is denoised via $\hat{\bx}_{0} = G_{\theta'}(\bx_{t}, t, c)$; noise is re-added to obtain $\bx_{t-1}$, and the procedure repeats until $t=1$, yielding $\bx_{0}$.

\begin{algorithm}[H]
	\caption{Sampling in OHGesture}
	\label{alg:sampling}
	\setlength{\baselineskip}{10pt}
	\begin{enumerate}
		\item Initialize with noise: $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$.
		\item Retrieve $\sqrt{\alpha_t}$, $\sqrt{1 - \alpha_t}$, and $\sqrt{\bar{\alpha}_t}$ from training; precompute $\sigma_t$ from $\alpha_t$ for each timestep $t: 1 \rightarrow T$.
		\item Split each 4-second speech segment into $\mathbf{a} \in \mathbb{R}^{64000}$.  
		      The initial seed gesture $\mathbf{s}$ is the data mean and is later updated from the inferred gesture segment.  
		      Select the desired emotion, obtain the transcript $\mathbf{v}$ from speech $\mathbf{a}$, and form the condition $c = [\mathbf{s}, \mathbf{e}, \mathbf{a}, \mathbf{v}]$.
		\item For each timestep, take $t$ \textbf{sequentially} from $[T, \dots, 1]$.
		\item Sample random noise $\mathbf{z} \sim \mathcal{N}(0, \mathbf{I})$.
		\item Infer $\hat{\mathbf{x}}_0^{(t)} = G_{\theta'}(\mathbf{x}_t, t, c)$.
		\item Diffuse $\hat{\mathbf{x}}_0^{(t)}$ from step $0 \rightarrow t$ to obtain $\hat{\mathbf{x}}_{t-1}^{(t)}$.
		\item Add noise: $\hat{\mathbf{x}}_{t-1} = \hat{\mathbf{x}}_{t-1}^{(t)} + \sigma_t \mathbf{z}$.
		\item Return to step 4.  
		      When $t = 1$, output the denoised gesture $\hat{\mathbf{x}}_0$.
	\end{enumerate}
\end{algorithm}

\autoref{alg:sampling} starts by initializing the noisy gesture $\mathbf{x}_T$ from $\mathcal{N}(0, \mathbf{I})$.  
The values $\sqrt{\alpha_t}$, $\sqrt{1-\alpha_t}$, and $\sqrt{\bar{\alpha}_t}$ obtained during training, together with $\sigma_t$, are employed at each timestep (1 … $T$).  
Each 4-second speech segment is represented by $\mathbf{a}$, and the seed gesture $\mathbf{s}$ is taken as the data mean or from the previously inferred segment.  
The desired emotion and the transcript form the condition $c = [\mathbf{s}, \mathbf{e}, \mathbf{a}, \mathbf{v}]$.  
The algorithm proceeds sequentially from $T$ to 1: random noise $\mathbf{z}$ is generated, the model predicts $\hat{\mathbf{x}}_0^{(t)}$ from $\mathbf{x}_t$, $t$, and $c$, then $\hat{\mathbf{x}}_{t-1}^{(t)}$ is computed and updated with noise.  
This loop continues until $t=1$, after which the algorithm outputs the final denoised gesture $\hat{\mathbf{x}}_0$.
